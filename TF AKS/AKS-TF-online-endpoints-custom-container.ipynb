{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a TensorFlow model served with TF Serving using a custom container in an online endpoint\n",
    "Learn how to deploy a custom container as an online endpoint in Azure Machine Learning.\n",
    "\n",
    "Custom container deployments can use web servers other than the default Python Flask server used by Azure Machine Learning. Users of these deployments can still take advantage of Azure Machine Learning's built-in monitoring, scaling, alerting, and authentication.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* To use Azure Machine Learning, you must have an Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).\n",
    "\n",
    "* Install and configure the [Python SDK v2](sdk/setup.sh).\n",
    "\n",
    "* You must have an Azure resource group, and you (or the service principal you use) must have Contributor access to it.\n",
    "\n",
    "* You must have an Azure Machine Learning workspace. \n",
    "\n",
    "* To deploy locally, you must install [Docker Engine](https://docs.docker.com/engine/install/) on your local computer. We highly recommend this option, so it's easier to debug issues.\n",
    "\n",
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Configure workspace details and get a handle to the workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name. We will use these details in the `MLClient` from `azure.ai.ml` to get a handle to the required Azure Machine Learning workspace. We use the default [default azure authentication](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python) for this tutorial. Check the [configuration notebook](../../jobs/configuration.ipynb) for more details on how to configure credentials and connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter details of your AML workspace\n",
    "subscription_id = \"\"\n",
    "resource_group = \"\"\n",
    "workspace = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download a TensorFlow model\n",
    "\n",
    "BASE_PATH=endpoints/online/custom-container\n",
    "\n",
    "AML_MODEL_NAME=tfserving-mounted\n",
    "\n",
    "MODEL_NAME=half_plus_two\n",
    "\n",
    "MODEL_BASE_PATH=/var/azureml-app/azureml-models/$AML_MODEL_NAME/1\n",
    "\n",
    "Download and unzip a model that divides an input by two and adds 2 to the result\n",
    "\n",
    "`wget https://aka.ms/half_plus_two-model -O $BASE_PATH/half_plus_two.tar.gz`\n",
    "\n",
    "`tar -xvf $BASE_PATH/half_plus_two.tar.gz -C $BASE_PATH`\n",
    "\n",
    "In in this sample, we have already downloaded the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test locally\n",
    "## 3.1 Use docker to run your image locally for testing\n",
    "Use docker to run your image locally for testing\n",
    "\n",
    "`docker run --rm -d -v $PWD/$BASE_PATH:$MODEL_BASE_PATH -p 8501:8501 \\\n",
    " -e MODEL_BASE_PATH=$MODEL_BASE_PATH -e MODEL_NAME=$MODEL_NAME \\\n",
    " --name=\"tfserving-test\" docker.io/tensorflow/serving:latest\n",
    "sleep 10`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Check that you can send liveness and scoring requests to the image\n",
    "First, check that the container is \"alive,\" meaning that the process inside the container is still running. You should get a 200 (OK) response.\n",
    "\n",
    "`curl -v http://localhost:8501/v1/models/$MODEL_NAME`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Check that you can get predictions about unlabeled data\n",
    "`curl --header \"Content-Type: application/json\" \\\n",
    "  --request POST \\\n",
    "  --data @$BASE_PATH/sample_request.json \\\n",
    "  http://localhost:8501/v1/models/$MODEL_NAME:predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Stop the image\n",
    "Now that you've tested locally, stop the image\n",
    "\n",
    "`docker stop tfserving-test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deploy your online endpoint to Azure\n",
    "Next, deploy your online endpoint to Azure.\n",
    "\n",
    "## 4.1 Configure online endpoint\n",
    "`endpoint_name`: The name of the endpoint. It must be unique in the Azure region. Naming rules are defined under [managed online endpoint limits](https://docs.microsoft.com/azure/machine-learning/how-to-manage-quotas#azure-machine-learning-managed-online-endpoints-preview).\n",
    "\n",
    "`auth_mode` : Use `key` for key-based authentication. Use `aml_token` for Azure Machine Learning token-based authentication. A `key` does not expire, but `aml_token` does expire. \n",
    "\n",
    "Optionally, you can add description, tags to your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
    "import datetime\n",
    "\n",
    "online_endpoint_name = \"k8s-endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = KubernetesOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    compute = \"akscluster\",\n",
    "    description=\"this is a sample online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\"foo\": \"bar\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Create the endpoint\n",
    "Using the `MLClient` created earlier, we will now create the Endpoint in the workspace. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpoint-03052139927515.canadacentral.inference.ml.azure.com/score', 'openapi_uri': 'https://endpoint-03052139927515.canadacentral.inference.ml.azure.com/swagger.json', 'name': 'endpoint-03052139927515', 'description': 'this is a sample online endpoint', 'tags': {'foo': 'bar'}, 'properties': {'createdBy': 'Jose Medina Gomez', 'createdAt': '2025-03-06T02:40:14.469007+0000', 'lastModifiedAt': '2025-03-06T02:40:14.469007+0000', 'azureml.onlineendpointid': '/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourcegroups/umiazmlaks/providers/microsoft.machinelearningservices/workspaces/umiazmlws/onlineendpoints/endpoint-03052139927515', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/providers/Microsoft.MachineLearningServices/locations/canadacentral/mfeOperationsStatus/oeidp:5158600d-81bf-4fd7-b9b8-a0df4ee2e6ca:ed9258a3-d2dc-494e-b716-78a328519b7d?api-version=2022-02-01-preview'}, 'print_as_yaml': False, 'id': '/subscriptions/14585b9f-5c83-4a76-8055-42149123f99f/resourceGroups/umiazmlaks/providers/Microsoft.MachineLearningServices/workspaces/umiazmlws/onlineEndpoints/endpoint-03052139927515', 'Resource__source_path': '', 'base_path': 'c:\\\\Users\\\\jomedin\\\\Downloads\\\\azureml-examples-main\\\\sdk\\\\python\\\\endpoints\\\\online\\\\custom-container', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001DB29AC7DA0>, 'auth_mode': 'key', 'location': 'canadacentral', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x000001DB2994DDF0>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Configure online deployment\n",
    "A deployment is a set of resources required for hosting the model that does the actual inferencing. We will create a deployment for our endpoint using the `ManagedOnlineDeployment` class.\n",
    "\n",
    "### Key aspects of deployment \n",
    "- `name` - Name of the deployment.\n",
    "- `endpoint_name` - Name of the endpoint to create the deployment under.\n",
    "- `model` - The model to use for the deployment. This value can be either a reference to an existing versioned model in the workspace or an inline model specification.\n",
    "- `environment` - The environment to use for the deployment. This value can be either a reference to an existing versioned environment in the workspace or an inline environment specification.\n",
    "- `code_configuration` - the configuration for the source code and scoring script\n",
    "    - `path`- Path to the source code directory for scoring the model\n",
    "    - `scoring_script` - Relative path to the scoring file in the source code directory\n",
    "- `instance_type` - The VM size to use for the deployment. For the list of supported sizes, see [Managed online endpoints SKU list](https://docs.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list).\n",
    "- `instance_count` - The number of instances to use for the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a blue deployment\n",
    "#model = Model(name=\"tfserving-mounted\", version=\"1\", path=\"half_plus_two\")\n",
    "model = ml_client.models.get(name=\"tfservingcustom\",version=\"1\")\n",
    "\n",
    "env = Environment(\n",
    "    image=\"docker.io/tensorflow/serving:latest\",\n",
    "    inference_config={\n",
    "        \"liveness_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two\"},\n",
    "        \"readiness_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two\"},\n",
    "        \"scoring_route\": {\"port\": 8501, \"path\": \"/v1/models/half_plus_two:predict\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "blue_deployment = KubernetesOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    environment=env,\n",
    "    environment_variables={\n",
    "        \"MODEL_BASE_PATH\": \"/var/azureml-app/azureml-models/tfservingcustom/1\",\n",
    "        \"MODEL_NAME\": \"half_plus_two\",\n",
    "    },\n",
    "    instance_count=1,\n",
    "    resources=ResourceRequirementsSettings(\n",
    "        requests=ResourceSettings(\n",
    "            cpu=\"100m\",\n",
    "            memory=\"0.5Gi\",\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readiness route vs. liveness route\n",
    "An HTTP server defines paths for both liveness and readiness. A liveness route is used to check whether the server is running. A readiness route is used to check whether the server is ready to do work. In machine learning inference, a server could respond 200 OK to a liveness request before loading a model. The server could respond 200 OK to a readiness request only after the model has been loaded into memory.\n",
    "\n",
    "Review the [Kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/) for more information about liveness and readiness probes.\n",
    "\n",
    "Notice that this deployment uses the same path for both liveness and readiness, since TF Serving only defines a liveness route."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Create the deployment\n",
    "Using the `MLClient` created earlier, we will now create the deployment in the workspace. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.begin_create_or_update(blue_deployment).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue deployment takes 100 traffic\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.begin_create_or_update(endpoint).result()\n",
    "\n",
    "#endpoint.traffic = {\"blue\": 0, \"green\": 0, \"orange\": 100}\n",
    "#ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test the endpoint with sample data\n",
    "Using the `MLClient` created earlier, we will get a handle to the endpoint. The endpoint can be invoked using the `invoke` command with the following parameters:\n",
    "- `endpoint_name` - Name of the endpoint\n",
    "- `request_file` - File with request data\n",
    "- `deployment_name` - Name of the specific deployment to test in an endpoint\n",
    "\n",
    "We will send a sample request using a [json](./model-1/sample-request.json) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"predictions\": [2.5, 3.0, 4.5\\n    ]\\n}'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the blue deployment with some sample data\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"orange\",\n",
    "    request_file=\"sample-request.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Managing endpoints and deployments\n",
    "\n",
    "## 6.1 Get details of the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the details for online endpoint\n",
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "# existing traffic details\n",
    "print(endpoint.traffic)\n",
    "\n",
    "# Get the scoring URI\n",
    "print(endpoint.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Get the logs for the new deployment\n",
    "Get the logs for the green deployment and verify as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instance status:\\nSystemSetup: Succeeded\\nUserContainerImagePull: Succeeded\\nModelDownload: Succeeded\\nUserContainerStart: Succeeded\\n\\nContainer events:\\nKind: Pod, Name: Pulling, Type: Normal, Time: 2025-03-06T03:03:59.790523Z, Message: Start pulling container image\\nKind: Pod, Name: Downloading, Type: Normal, Time: 2025-03-06T03:04:00.643373Z, Message: Start downloading models\\nKind: Pod, Name: Pulled, Type: Normal, Time: 2025-03-06T03:04:20.93283Z, Message: Container image is pulled successfully\\nKind: Pod, Name: Downloaded, Type: Normal, Time: 2025-03-06T03:04:20.93283Z, Message: Models are downloaded successfully\\nKind: Pod, Name: Created, Type: Normal, Time: 2025-03-06T03:04:21.006594Z, Message: Created container inference-server\\nKind: Pod, Name: Started, Type: Normal, Time: 2025-03-06T03:04:21.077718Z, Message: Started container inference-server\\nKind: Pod, Name: ContainerReady, Type: Normal, Time: 2025-03-06T03:04:36.365197858Z, Message: Container is ready\\n\\nContainer logs:\\n2025-03-06 03:04:21.132791: I tensorflow_serving/model_servers/server.cc:77] Building single TensorFlow model file config:  model_name: half_plus_two model_base_path: /var/azureml-app/azureml-models/tfserving-mounted/1/half_plus_two\\n2025-03-06 03:04:21.133170: I tensorflow_serving/model_servers/server_core.cc:481] Adding/updating models.\\n2025-03-06 03:04:21.133206: I tensorflow_serving/model_servers/server_core.cc:610]  (Re-)adding model: half_plus_two\\n2025-03-06 03:04:21.339183: I tensorflow_serving/core/basic_manager.cc:771] Successfully reserved resources to load servable {name: half_plus_two version: 123}\\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\\nI0000 00:00:1741230261.339537      13 loader_harness.cc:71] Approving load for servable version {name: half_plus_two version: 123}\\nI0000 00:00:1741230261.339772      13 loader_harness.cc:79] Loading servable version {name: half_plus_two version: 123}\\n2025-03-06 03:04:21.339959: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/azureml-app/azureml-models/tfserving-mounted/1/half_plus_two/00000123\\n2025-03-06 03:04:21.340496: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\\n2025-03-06 03:04:21.341535: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/azureml-app/azureml-models/tfserving-mounted/1/half_plus_two/00000123\\n2025-03-06 03:04:21.341839: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\\nI0000 00:00:1741230261.369818      13 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\\n2025-03-06 03:04:21.370932: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\\n2025-03-06 03:04:21.388848: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/azureml-app/azureml-models/tfserving-mounted/1/half_plus_two/00000123\\n2025-03-06 03:04:21.394089: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 54134 microseconds.\\n2025-03-06 03:04:21.394419: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:82] No warmup data file found at /var/azureml-app/azureml-models/tfserving-mounted/1/half_plus_two/00000123/assets.extra/tf_serving_warmup_requests\\nI0000 00:00:1741230261.518877      13 loader_harness.cc:100] Successfully loaded servable version {name: half_plus_two version: 123}\\n2025-03-06 03:04:21.519765: I tensorflow_serving/model_servers/server_core.cc:502] Finished adding/updating models\\n2025-03-06 03:04:21.520005: I tensorflow_serving/model_servers/server.cc:121] Using InsecureServerCredentials\\n2025-03-06 03:04:21.520140: I tensorflow_serving/model_servers/server.cc:388] Profiler service is enabled\\n2025-03-06 03:04:21.523003: I tensorflow_serving/model_servers/server.cc:423] Running gRPC ModelServer at 0.0.0.0:8500 ...\\n2025-03-06 03:04:21.523707: I tensorflow_serving/model_servers/server.cc:444] Exporting HTTP/REST API at:localhost:8501 ...\\n[evhttp_server.cc : 250] NET_LOG: Entering the event loop ...\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_deployments.get_logs(\n",
    "    name=\"blue\", endpoint_name=online_endpoint_name, lines=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Deploy a custom container as an online endpoint. Use web servers other than the default Python Flask server used by Azure ML without losing the benefits of Azure ML's built-in monitoring, scaling, alerting, and authentication."
  },
  "kernelspec": {
   "display_name": "secdemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
